{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atchyuteswar/GPT_2_Fine_Tuning/blob/main/GPT_2_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Required Packages\n",
        "!pip install transformers datasets accelerate pandas -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install torch -q\n",
        "!pip install ipywidgets -q\n",
        "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "!pip install optimum[intel] -q # If you plan to use quantization later\n",
        "!pip install neural_compressor -q # If you plan to use quantization later\n",
        "print(\"IMPORTANT: After installation, go to 'Runtime' -> 'Restart runtime' from the Colab menu. Then, run all cells from the beginning.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_XV1OGJkowj",
        "outputId": "b7511b3f-2835-4069-ebe0-97f050a567f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hEnabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.6/342.6 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mIMPORTANT: After installation, go to 'Runtime' -> 'Restart runtime' from the Colab menu. Then, run all cells from the beginning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 1: Setup and Installations (Updated for numpy compatibility)\n",
        "\n",
        "# Force uninstall potentially conflicting packages\n",
        "!pip uninstall numpy pandas -y\n",
        "!pip uninstall transformers datasets accelerate torch -y # Uninstalling these too for a clean slate\n",
        "\n",
        "# Reinstall everything from scratch, ensuring compatibility\n",
        "# We'll install numpy first, then pandas, then torch, then huggingface libraries\n",
        "!pip install numpy==1.26.4 -q # Pin to a known stable numpy version for better compatibility\n",
        "!pip install pandas -q\n",
        "!pip install torch -q\n",
        "!pip install transformers datasets accelerate -q\n",
        "\n",
        "# SentencePiece is often a dependency for various tokenizers, good to have\n",
        "!pip install sentencepiece -q\n",
        "\n",
        "# For progress bars during training (optional, but helpful)\n",
        "!pip install ipywidgets -q\n",
        "!jupyter nbextension enable --py widgetsnbextension --sys-prefix\n",
        "\n",
        "# Libraries for model quantization (for CPU optimization)\n",
        "# If you plan to use quantization, make sure these are installed AFTER core libraries\n",
        "# and consider if they introduce new numpy dependencies that might conflict.\n",
        "# If you run into numpy errors again with these, try installing them first before torch/transformers.\n",
        "# !pip install optimum[intel] -q\n",
        "# !pip install neural_compressor -q\n",
        "\n",
        "print(\"All requested packages are being re-installed. Please wait for completion.\")\n",
        "print(\"\\nIMPORTANT: After installation, go to 'Runtime' -> 'Restart runtime' from the Colab menu.\")\n",
        "print(\"Then, run all cells from the beginning, checking the output of Cell 5 carefully.\")\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import random\n",
        "import numpy as np # Import numpy here as well\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"Using device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
      ],
      "metadata": {
        "id": "fdFGtaaenZy0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "53d4c3d2-45dc-4247-901a-ca7610bd92f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Found existing installation: pandas 2.2.2\n",
            "Uninstalling pandas-2.2.2:\n",
            "  Successfully uninstalled pandas-2.2.2\n",
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "Found existing installation: datasets 4.0.0\n",
            "Uninstalling datasets-4.0.0:\n",
            "  Successfully uninstalled datasets-4.0.0\n",
            "Found existing installation: accelerate 1.9.0\n",
            "Uninstalling accelerate-1.9.0:\n",
            "  Successfully uninstalled accelerate-1.9.0\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optimum-intel 1.23.0 requires datasets>=1.4.0, which is not installed.\n",
            "optimum-intel 1.23.0 requires torch>=1.11, which is not installed.\n",
            "optimum-intel 1.23.0 requires transformers<4.52,>=4.36, which is not installed.\n",
            "neural-compressor 3.4.1 requires pandas, which is not installed.\n",
            "optimum 1.27.0 requires torch>=1.11, which is not installed.\n",
            "optimum 1.27.0 requires transformers>=4.29, which is not installed.\n",
            "tsfresh 0.21.0 requires pandas>=0.25.0, which is not installed.\n",
            "dopamine-rl 4.1.2 requires pandas>=0.24.2, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires pandas, which is not installed.\n",
            "bokeh 3.7.3 requires pandas>=1.2, which is not installed.\n",
            "pymc 5.25.1 requires pandas>=0.24.0, which is not installed.\n",
            "geopandas 1.1.1 requires pandas>=2.0.0, which is not installed.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, which is not installed.\n",
            "prophet 1.1.7 requires pandas>=1.0.4, which is not installed.\n",
            "gradio 5.38.2 requires pandas<3.0,>=1.0, which is not installed.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, which is not installed.\n",
            "pandas-gbq 0.29.2 requires pandas>=1.1.4, which is not installed.\n",
            "yfinance 0.2.65 requires pandas>=1.3.0, which is not installed.\n",
            "arviz 0.22.0 requires pandas>=2.1.0, which is not installed.\n",
            "statsmodels 0.14.5 requires pandas!=2.1.0,>=1.4, which is not installed.\n",
            "torchtune 0.6.1 requires datasets, which is not installed.\n",
            "peft 0.16.0 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.16.0 requires torch>=1.13.0, which is not installed.\n",
            "peft 0.16.0 requires transformers, which is not installed.\n",
            "holoviews 1.21.0 requires pandas>=1.3, which is not installed.\n",
            "dask-cuda 25.6.0 requires pandas>=1.3, which is not installed.\n",
            "cufflinks 0.17.3 requires pandas>=0.19.2, which is not installed.\n",
            "mlxtend 0.23.4 requires pandas>=0.24.2, which is not installed.\n",
            "panel 1.7.5 requires pandas>=1.2, which is not installed.\n",
            "bigframes 2.12.0 requires pandas>=1.5.3, which is not installed.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, which is not installed.\n",
            "shap 0.48.0 requires pandas, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires pandas>=1.1.4, which is not installed.\n",
            "bqplot 0.12.45 requires pandas<3.0.0,>=1.0.0, which is not installed.\n",
            "cmdstanpy 1.2.5 requires pandas, which is not installed.\n",
            "geemap 0.35.3 requires pandas, which is not installed.\n",
            "db-dtypes 1.4.3 requires pandas>=1.5.3, which is not installed.\n",
            "seaborn 0.13.2 requires pandas>=1.2, which is not installed.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, which is not installed.\n",
            "libpysal 4.13.0 requires pandas>=1.4, which is not installed.\n",
            "xarray 2025.7.1 requires pandas>=2.2, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m123.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optimum-intel 1.23.0 requires datasets>=1.4.0, which is not installed.\n",
            "optimum-intel 1.23.0 requires transformers<4.52,>=4.36, which is not installed.\n",
            "optimum 1.27.0 requires transformers>=4.29, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\n",
            "peft 0.16.0 requires accelerate>=0.21.0, which is not installed.\n",
            "peft 0.16.0 requires transformers, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m114.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optimum-intel 1.23.0 requires transformers<4.52,>=4.36, but you have transformers 4.54.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mEnabling notebook extension jupyter-js-widgets/extension...\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "Paths used for configuration of notebook: \n",
            "    \t\n",
            "      - Validating: \u001b[32mOK\u001b[0m\n",
            "Paths used for configuration of notebook: \n",
            "    \t/usr/etc/jupyter/nbconfig/notebook.json\n",
            "All requested packages are being re-installed. Please wait for completion.\n",
            "\n",
            "IMPORTANT: After installation, go to 'Runtime' -> 'Restart runtime' from the Colab menu.\n",
            "Then, run all cells from the beginning, checking the output of Cell 5 carefully.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3198661192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorForLanguageModeling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 2: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create a directory for your project\n",
        "project_path = '/content/drive/My Drive/EmotionalSupportBot-4'\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "print(f\"Project directory created at: {project_path}\")"
      ],
      "metadata": {
        "id": "cpetb5hCngzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset as HFDataset\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "\n",
        "# Define your stop token globally\n",
        "STOP_TOKEN = \"<|stop|>\"\n",
        "\n",
        "def get_bot_response_options(emotion):\n",
        "    \"\"\"\n",
        "    Returns a list of diverse, empathetic bot response options for a given emotion,\n",
        "    prioritizing sentences and varied conversational flow, ending with STOP_TOKEN.\n",
        "    Strictly no [EMOTION:] or [EMOTE:] or 'comments' in the bot's actual response.\n",
        "    \"\"\"\n",
        "    responses = {\n",
        "        \"joy\": [\n",
        "            f\"That's absolutely wonderful to hear! What's been the best part of your day?{STOP_TOKEN}\",\n",
        "            f\"I'm so thrilled you're feeling joyful! Tell me all about what's making you smile.{STOP_TOKEN}\",\n",
        "            f\"What fantastic news! It sounds like you're having a truly great day. Share what's contributing to this feeling.{STOP_TOKEN}\",\n",
        "            f\"Your happiness is contagious! I'd love to hear what sparked this wonderful emotion for you.{STOP_TOKEN}\",\n",
        "            f\"It's truly heartwarming to know you're feeling cheerful. What's been particularly uplifting for you?{STOP_TOKEN}\",\n",
        "            f\"That's brilliant! What a beautiful feeling to have. Share more if you feel like it.{STOP_TOKEN}\",\n",
        "            f\"I'm so glad to hear this! What aspect of your experience is making you feel this way?{STOP_TOKEN}\",\n",
        "            f\"Awesome! It's always great to hear about positive feelings. What are you enjoying most about this?{STOP_TOKEN}\",\n",
        "            f\"You sound genuinely happy! What's going on that has you feeling this way?{STOP_TOKEN}\",\n",
        "            f\"That's a lovely feeling to express. Tell me more about what's bringing you such joy.{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"sadness\": [\n",
        "            f\"I hear that you're feeling a deep sadness right now. It's truly okay to feel this way, and I'm here for you.{STOP_TOKEN}\",\n",
        "            f\"I'm so sorry to hear you're feeling down. Would you like to talk more about what's weighing on your mind?{STOP_TOKEN}\",\n",
        "            f\"It sounds like you're going through a very tough time. Please know I'm listening, and you don't have to carry this alone.{STOP_TOKEN}\",\n",
        "            f\"A heavy heart can be incredibly challenging. If you feel up to it, I'm here to listen to anything you want to share.{STOP_TOKEN}\",\n",
        "            f\"It takes courage to acknowledge sadness. What might be contributing to these feelings for you today?{STOP_TOKEN}\",\n",
        "            f\"I'm sending you virtual support. What's making you feel this way?{STOP_TOKEN}\",\n",
        "            f\"It sounds like a difficult moment. I'm here to listen, without judgment, about what's on your mind.{STOP_TOKEN}\",\n",
        "            f\"I understand you're feeling low. Is there anything specific that's bothering you right now?{STOP_TOKEN}\",\n",
        "            f\"Sometimes it really helps to talk things through. I'm ready to hear what you need to say.{STOP_TOKEN}\",\n",
        "            f\"I'm sorry you're hurting. What's the main thing causing you distress?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"anger\": [\n",
        "            f\"It sounds like you're feeling a lot of anger right now. What's upsetting you?{STOP_TOKEN}\",\n",
        "            f\"I can hear the frustration in your words. Tell me more about what happened to make you feel this way.{STOP_TOKEN}\",\n",
        "            f\"It's completely valid to feel angry sometimes. I'm here to hear you out without judgment.{STOP_TOKEN}\",\n",
        "            f\"What is it that's making your blood boil? I'm listening to your thoughts.{STOP_TOKEN}\",\n",
        "            f\"I understand you're feeling furious. Can you describe the situation that led to this intense feeling?{STOP_TOKEN}\",\n",
        "            f\"It sounds like something really got under your skin. What exactly is on your mind?{STOP_TOKEN}\",\n",
        "            f\"Anger is a powerful emotion. What sparked this feeling for you?{STOP_TOKEN}\",\n",
        "            f\"I'm ready to listen to what's making you feel so mad. Please share.{STOP_TOKEN}\",\n",
        "            f\"What's the core issue that's causing this anger for you?{STOP_TOKEN}\",\n",
        "            f\"It's okay to express your anger. What do you need to get off your chest?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"fear\": [\n",
        "            f\"It sounds like you're feeling scared or anxious. What's on your mind that's causing this fear?{STOP_TOKEN}\",\n",
        "            f\"I'm here to help you through this fear. Can you tell me more about what's bothering you and making you feel unsafe?{STOP_TOKEN}\",\n",
        "            f\"It's understandable to feel afraid. What specific thoughts or situations are making you feel this way?{STOP_TOKEN}\",\n",
        "            f\"A knot of anxiety can be overwhelming. What are you most worried about right now?{STOP_TOKEN}\",\n",
        "            f\"I understand that feeling of apprehension. What's the source of your fear?{STOP_TOKEN}\",\n",
        "            f\"What is it that's causing you to feel so afraid at this moment?{STOP_TOKEN}\",\n",
        "            f\"Fear can be paralyzing. I'm here to listen to your concerns, please share them.{STOP_TOKEN}\",\n",
        "            f\"It sounds like you're dealing with a lot of uncertainty. What's making you feel uneasy?{STOP_TOKEN}\",\n",
        "            f\"I'm here to offer support. What is it that truly frightens you?{STOP_TOKEN}\",\n",
        "            f\"You sound really scared. What's happening that's causing this?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"surprise\": [\n",
        "            f\"Oh, you're genuinely surprised! Is it a pleasant surprise or something unexpected that's causing concern?{STOP_TOKEN}\",\n",
        "            f\"Wow, that sounds completely unexpected! Tell me more about what surprised you.{STOP_TOKEN}\",\n",
        "            f\"That's quite a revelation! How are you feeling about this sudden turn of events?{STOP_TOKEN}\",\n",
        "            f\"I'm curious to know more about this surprise! What was the moment you realized it?{STOP_TOKEN}\",\n",
        "            f\"That's astonishing! What specific detail caught you off guard the most?{STOP_TOKEN}\",\n",
        "            f\"What a twist! Tell me all about what happened.{STOP_TOKEN}\",\n",
        "            f\"I never saw that coming! How did you react to that?{STOP_TOKEN}\",\n",
        "            f\"That must have been a shock! How are you processing it now?{STOP_TOKEN}\",\n",
        "            f\"Oh my goodness! What was the biggest surprise for you?{STOP_TOKEN}\",\n",
        "            f\"You sound surprised! Was it a good surprise or a bad one?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"disgust\": [\n",
        "            f\"It sounds like you're feeling a strong sense of disgust. What is it that's bothering you so much?{STOP_TOKEN}\",\n",
        "            f\"I understand that feeling of revulsion. Would you like to elaborate on what's making you feel this way?{STOP_TOKEN}\",\n",
        "            f\"That's a very strong reaction. What specific aspect of the situation is causing you such deep disgust?{STOP_TOKEN}\",\n",
        "            f\"I can sense your profound aversion. What did you witness or experience that made you feel this way?{STOP_TOKEN}\",\n",
        "            f\"It sounds truly unpleasant. Tell me what's making you feel utterly disgusted.{STOP_TOKEN}\",\n",
        "            f\"What's making you feel so repulsed by this?{STOP_TOKEN}\",\n",
        "            f\"That's a powerful feeling to have. What triggered it for you?{STOP_TOKEN}\",\n",
        "            f\"I'm listening to what you find so off-putting. Please share.{STOP_TOKEN}\",\n",
        "            f\"It sounds like something really bothered you. What exactly is it?{STOP_TOKEN}\",\n",
        "            f\"What happened that made you feel this strong sense of disgust?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"love\": [\n",
        "            f\"That's a beautiful feeling to express! Who or what are you feeling love for today?{STOP_TOKEN}\",\n",
        "            f\"My heart is full of love, that's wonderful! What aspects of this love are you cherishing most?{STOP_TOKEN}\",\n",
        "            f\"It's truly inspiring to hear you're experiencing love. What makes this feeling so special for you?{STOP_TOKEN}\",\n",
        "            f\"You're radiating warmth! Tell me more about what fills your heart with love.{STOP_TOKEN}\",\n",
        "            f\"Love is a powerful and uplifting emotion. What is it that's bringing you this profound sense of affection?{STOP_TOKEN}\",\n",
        "            f\"That's a lovely sentiment. What brings you this feeling of love?{STOP_TOKEN}\",\n",
        "            f\"It's wonderful to hear you're filled with love. Who are you thinking of right now?{STOP_TOKEN}\",\n",
        "            f\"You sound very happy and connected. What's making you feel so much affection?{STOP_TOKEN}\",\n",
        "            f\"Love is a wonderful thing. Tell me about what you cherish in your life.{STOP_TOKEN}\",\n",
        "            f\"That's a beautiful feeling. What's on your mind that's bringing this sense of love?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"gratitude\": [\n",
        "            f\"It's great to hear you're feeling grateful! What specifically are you thankful for today?{STOP_TOKEN}\",\n",
        "            f\"Thank you for sharing your appreciation! What kindness or situation has filled you with gratitude?{STOP_TOKEN}\",\n",
        "            f\"That's a lovely sentiment. Sharing what you're grateful for can be very uplifting. What's on your mind?{STOP_TOKEN}\",\n",
        "            f\"I can feel your thankfulness! What particular acts or blessings are you appreciating right now?{STOP_TOKEN}\",\n",
        "            f\"Gratitude is a wonderful feeling. Tell me about what's making you feel so appreciative.{STOP_TOKEN}\",\n",
        "            f\"What blessings are you counting today that fill you with gratitude?{STOP_TOKEN}\",\n",
        "            f\"It's wonderful to hear you're thankful. What's making you feel this way?{STOP_TOKEN}\",\n",
        "            f\"What are you most appreciative of right now?{STOP_TOKEN}\",\n",
        "            f\"You sound very grateful. Tell me what's inspiring this feeling for you.{STOP_TOKEN}\",\n",
        "            f\"That's lovely. What are you thankful for in this moment?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"anxiety\": [\n",
        "            f\"It sounds like you're feeling anxious. What thoughts are weighing on you most heavily right now?{STOP_TOKEN}\",\n",
        "            f\"I understand that feeling of nervousness. Perhaps talking about what's causing your anxiety will help.{STOP_TOKEN}\",\n",
        "            f\"Anxiety can be incredibly tough to navigate. I'm here to listen without judgment about what's making you feel on edge.{STOP_TOKEN}\",\n",
        "            f\"Your mind seems to be racing with worries. What specific concerns are contributing to this anxious feeling?{STOP_TOKEN}\",\n",
        "            f\"It's okay to feel overwhelmed by anxiety. What's making you feel so uneasy or stressed?{STOP_TOKEN}\",\n",
        "            f\"What's causing you the most worry right now?{STOP_TOKEN}\",\n",
        "            f\"I'm here to listen to your anxious thoughts. What's on your mind?{STOP_TOKEN}\",\n",
        "            f\"It sounds like you're carrying a lot of stress. What's contributing to it for you?{STOP_TOKEN}\",\n",
        "            f\"What are your biggest concerns at the moment?{STOP_TOKEN}\",\n",
        "            f\"You sound very overwhelmed. What's happening that's causing this feeling?{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"hope\": [\n",
        "            f\"That's wonderful to have hope! What are you hoping for, and what makes you optimistic about it?{STOP_TOKEN}\",\n",
        "            f\"There's a beautiful glimmer of hope in your words. Tell me more about what gives you this positive outlook.{STOP_TOKEN}\",\n",
        "            f\"Hope can be a powerful motivator. What specific aspirations or beliefs are fueling your hopeful feelings?{STOP_TOKEN}\",\n",
        "            f\"I'm inspired by your sense of hope! What future possibilities are you looking forward to or working towards?{STOP_TOKEN}\",\n",
        "            f\"It's truly uplifting to hear you're feeling hopeful. What positive changes or outcomes do you envision?{STOP_TOKEN}\",\n",
        "            f\"What are you wishing for that makes you feel hopeful?{STOP_TOKEN}\",\n",
        "            f\"It's great to have something to look forward to. What gives you hope for the future?{STOP_TOKEN}\",\n",
        "            f\"You sound very positive about what's to come. What's making you feel that way?{STOP_TOKEN}\",\n",
        "            f\"What's inspiring your sense of hope today?{STOP_TOKEN}\",\n",
        "            f\"Hope is a wonderful feeling. Tell me about what gives you strength and optimism.{STOP_TOKEN}\"\n",
        "        ],\n",
        "        \"neutral\": [\n",
        "            f\"Okay, I hear you. What else is on your mind today?{STOP_TOKEN}\",\n",
        "            f\"Thanks for sharing. Is there anything else you'd like to discuss or ask about?{STOP_TOKEN}\",\n",
        "            f\"I'm here to chat if you need anything. What's next on your agenda?{STOP_TOKEN}\",\n",
        "            f\"Understood. Is there anything specific you were looking for help with right now?{STOP_TOKEN}\",\n",
        "            f\"I'm ready to listen. What would you like to talk about today?{STOP_TOKEN}\",\n",
        "            f\"Alright. How can I assist you further?{STOP_TOKEN}\",\n",
        "            f\"What's going on with you today?{STOP_TOKEN}\",\n",
        "            f\"I'm here if you want to share more about anything.{STOP_TOKEN}\",\n",
        "            f\"No specific emotion detected, but I'm here to listen if you need to talk.{STOP_TOKEN}\",\n",
        "            f\"I understand. What else is on your mind that you'd like to explore?{STOP_TOKEN}\"\n",
        "        ]\n",
        "    }\n",
        "    return random.choice(responses[emotion]) if emotion in responses else random.choice(responses[\"neutral\"])\n",
        "\n",
        "def generate_user_sentence(emotion):\n",
        "    \"\"\"\n",
        "    Generates synthetic user sentences for a given emotion.\n",
        "    These are kept relatively simple as the focus is on bot response diversity.\n",
        "    \"\"\"\n",
        "    templates = {\n",
        "        \"joy\": [\n",
        "            \"I'm feeling incredibly happy today!\", \"This news brings me so much joy.\", \"Everything is going perfectly, I'm delighted!\",\n",
        "            \"What a wonderful day, I feel so cheerful.\", \"I'm on top of the world!\", \"I can't stop smiling, this is amazing.\",\n",
        "            \"Such a joyous occasion!\", \"I feel so uplifted and content.\", \"I'm having a great day!\", \"Life is good.\"\n",
        "        ],\n",
        "        \"sadness\": [\n",
        "            \"I feel really down today.\", \"This makes me incredibly sad.\", \"I'm struggling to find any happiness right now.\",\n",
        "            \"A deep sense of sorrow washes over me.\", \"I wish things were different, I feel so low.\", \"It's a tough day, I'm feeling heartbroken.\",\n",
        "            \"I just want to cry.\", \"Everything seems so bleak.\", \"I'm going through a hard time.\", \"I feel so alone.\"\n",
        "        ],\n",
        "        \"anger\": [\n",
        "            \"I'm absolutely furious about this!\", \"This situation makes me so mad.\", \"I can't believe they did that, it's infuriating.\",\n",
        "            \"I'm seething with rage.\", \"My blood is boiling.\", \"I feel so frustrated and resentful.\",\n",
        "            \"This is unacceptable!\", \"I'm so angry I could scream.\", \"I'm fed up with everything.\", \"This makes me so annoyed.\"\n",
        "        ],\n",
        "        \"fear\": [\n",
        "            \"I'm so scared right now.\", \"This situation is terrifying.\", \"I feel a knot of anxiety in my stomach.\",\n",
        "            \"What if something bad happens?\", \"I'm constantly worried and apprehensive.\", \"A chilling fear grips me.\",\n",
        "            \"I'm afraid of what's to come.\", \"This uncertainty is frightening.\", \"I'm really nervous about this.\", \"I'm afraid.\"\n",
        "        ],\n",
        "        \"surprise\": [\n",
        "            \"Wow, I'm genuinely surprised!\", \"I never saw that coming, what a shock!\", \"This is completely unexpected.\",\n",
        "            \"Oh my goodness, I'm astonished!\", \"That's quite a revelation.\", \"I'm taken aback by this.\",\n",
        "            \"What a twist!\", \"This caught me off guard.\", \"I'm truly amazed!\", \"I didn't expect that.\"\n",
        "        ],\n",
        "        \"disgust\": [\n",
        "            \"That's absolutely repulsive.\", \"I feel sick to my stomach.\", \"This is so gross.\",\n",
        "            \"I'm filled with revulsion.\", \"I can't stand the sight of it.\", \"This is truly abominable.\",\n",
        "            \"It makes me want to gag.\", \"I find this utterly disgusting.\", \"That's sickening.\", \"I feel nauseous.\"\n",
        "        ],\n",
        "        \"love\": [\n",
        "            \"I deeply love you.\", \"My heart is full of love.\", \"I cherish our time together.\",\n",
        "            \"I feel so much affection for them.\", \"This is what true love feels like.\", \"I adore this feeling.\",\n",
        "            \"You mean the world to me.\", \"I'm so fond of them.\", \"I love my family.\", \"I feel so much affection.\"\n",
        "        ],\n",
        "        \"gratitude\": [\n",
        "            \"I'm so grateful for your help.\", \"Thank you so much, I really appreciate it.\", \"I feel truly blessed.\",\n",
        "            \"Your kindness means a lot to me.\", \"I'm thankful for everything.\", \"I can't express how appreciative I am.\",\n",
        "            \"This fills me with immense gratitude.\", \"I owe you a big thank you.\", \"I'm so thankful for this.\", \"I appreciate you.\"\n",
        "        ],\n",
        "        \"anxiety\": [\n",
        "            \"I'm feeling very anxious about the future.\", \"My mind is racing with worries.\", \"I can't seem to calm down, I'm so stressed.\",\n",
        "            \"A constant sense of dread is with me.\", \"I'm overwhelmed by all these thoughts.\", \"I feel on edge and nervous.\",\n",
        "            \"The uncertainty is making me so uneasy.\", \"I'm having trouble breathing due to stress.\", \"I'm so worried.\", \"I feel restless.\"\n",
        "        ],\n",
        "        \"hope\": [\n",
        "            \"I'm hopeful that things will get better.\", \"There's a glimmer of hope on the horizon.\", \"I believe in a brighter future.\",\n",
        "            \"I'm optimistic about the outcome.\", \"I wish for the best.\", \"I'm holding onto hope.\",\n",
        "            \"May good things come our way.\", \"I have a positive outlook.\", \"I feel a sense of hope.\", \"I'm optimistic.\"\n",
        "        ],\n",
        "        \"neutral\": [\n",
        "            \"The sky is blue.\", \"I am going to the market.\", \"The cat sat on the mat.\",\n",
        "            \"It's a normal day.\", \"I need to buy groceries.\", \"The book is on the table.\",\n",
        "            f\"I'm thinking about dinner. It's {pd.Timestamp.now().strftime('%I:%M %p IST on %A, %B %d, %Y in Dundigal, Telangana, India.')}\", # Acknowledge current context\n",
        "            \"The weather is good.\", \"I'm doing okay.\", \"Just a regular day.\", \"What's up?\"\n",
        "        ]\n",
        "    }\n",
        "    return random.choice(templates[emotion]) if emotion in templates else random.choice(templates[\"neutral\"])\n",
        "\n",
        "\n",
        "def create_and_format_emotion_dataset(\n",
        "    num_synthetic_per_emotion=250,\n",
        "    total_dataset_size=5000,\n",
        "    project_path='/content/drive/My Drive/EmotionalSupportBot-4' # New path for this version\n",
        "):\n",
        "    \"\"\"\n",
        "    Creates an emotion dataset with user_input, emotion, and assistant_response fields,\n",
        "    outputs to JSONL, and dynamically formats for fine-tuning.\n",
        "    \"\"\"\n",
        "    all_structured_data = [] # Store dictionaries here\n",
        "    emotion_labels = [\n",
        "        \"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\",\n",
        "        \"love\", \"gratitude\", \"anxiety\", \"hope\", \"neutral\"\n",
        "    ]\n",
        "\n",
        "    # 1. Load an existing emotion dataset (dair-ai/emotion)\n",
        "    print(\"Attempting to load existing 'dair-ai/emotion' dataset from Hugging Face...\")\n",
        "    try:\n",
        "        hf_emotion_dataset = load_dataset(\"dair-ai/emotion\", split=\"train\")\n",
        "        id_to_emotion = {\n",
        "            0: \"sadness\", 1: \"joy\", 2: \"love\", 3: \"anger\", 4: \"fear\", 5: \"surprise\"\n",
        "        }\n",
        "        for item in hf_emotion_dataset:\n",
        "            user_text = item['text']\n",
        "            emotion_label = id_to_emotion.get(item['label'], \"neutral\")\n",
        "            assistant_response = get_bot_response_options(emotion_label) # Get a balanced response\n",
        "            all_structured_data.append({\n",
        "                \"user_input\": user_text,\n",
        "                \"emotion\": emotion_label,\n",
        "                \"assistant_response\": assistant_response # This already includes <|stop|>\n",
        "            })\n",
        "        print(f\"Loaded {len(hf_emotion_dataset)} samples from 'dair-ai/emotion' and formatted them.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load 'dair-ai/emotion' dataset. Error: {e}\")\n",
        "        print(\"Continuing with only synthetic data generation.\")\n",
        "\n",
        "    # 2. Generate synthetic data to broaden emotion coverage and reach target size\n",
        "    print(f\"Generating {num_synthetic_per_emotion} enhanced synthetic conversational pairs per emotion...\")\n",
        "    for emotion_label in emotion_labels:\n",
        "        for _ in range(num_synthetic_per_emotion):\n",
        "            user_text = generate_user_sentence(emotion_label)\n",
        "            assistant_response = get_bot_response_options(emotion_label) # Get a balanced response\n",
        "            all_structured_data.append({\n",
        "                \"user_input\": user_text,\n",
        "                \"emotion\": emotion_label,\n",
        "                \"assistant_response\": assistant_response # This already includes <|stop|>\n",
        "            })\n",
        "\n",
        "    # Shuffle and trim to desired size\n",
        "    random.shuffle(all_structured_data) # Shuffle the list of dictionaries\n",
        "    if len(all_structured_data) > total_dataset_size:\n",
        "        all_structured_data = random.sample(all_structured_data, total_dataset_size) # Random sample\n",
        "    elif len(all_structured_data) < total_dataset_size:\n",
        "        print(f\"Warning: Could not reach {total_dataset_size} samples. Current size: {len(all_structured_data)}\")\n",
        "        print(\"Consider increasing 'num_synthetic_per_emotion' or finding more external datasets.\")\n",
        "\n",
        "    print(f\"Final structured dataset size: {len(all_structured_data)} samples.\")\n",
        "    print(\"\\nExample structured data from the dataset (first 2 samples):\")\n",
        "    for i in range(min(2, len(all_structured_data))): # Print 2 samples for clarity\n",
        "        print(f\"- {all_structured_data[i]}\")\n",
        "\n",
        "    # Save the dataset to Google Drive as JSONL\n",
        "    os.makedirs(project_path, exist_ok=True)\n",
        "    dataset_file_jsonl = os.path.join(project_path, \"emotional_support_structured_dataset.jsonl\")\n",
        "\n",
        "    with open(dataset_file_jsonl, 'w', encoding='utf-8') as f:\n",
        "        for entry in all_structured_data:\n",
        "            json.dump(entry, f)\n",
        "            f.write('\\n')\n",
        "\n",
        "    print(f\"\\nStructured dataset saved to: {dataset_file_jsonl}\")\n",
        "\n",
        "    # For fine-tuning, we'll need to re-format this into a single 'text' string\n",
        "    # We return the raw structured data here, and format it in the next step.\n",
        "    return HFDataset.from_list(all_structured_data)\n",
        "\n",
        "# --- Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Mount Google Drive (if running in Colab)\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        project_root = '/content/drive/My Drive/EmotionalSupportBot-4' # New directory for this version\n",
        "    except ImportError:\n",
        "        print(\"Not in Google Colab environment. Assuming local execution.\")\n",
        "        project_root = './EmotionalSupportBot-4' # Adjust if running locally and want to save elsewhere\n",
        "\n",
        "    dataset_hf_structured = create_and_format_emotion_dataset(\n",
        "        num_synthetic_per_emotion=250,\n",
        "        total_dataset_size=5000,\n",
        "        project_path=project_root\n",
        "    )\n",
        "\n",
        "    print(\"\\nDataset generation complete. You can now use 'emotional_support_structured_dataset.jsonl' for fine-tuning.\")\n",
        "    print(\"The returned object 'dataset_hf_structured' is a Hugging Face Dataset with 'user_input', 'emotion', 'assistant_response' columns.\")"
      ],
      "metadata": {
        "id": "DhjQJksCnmG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 3: Load Enhanced Dataset (from JSONL and reformat for training)\n",
        "import pandas as pd\n",
        "from datasets import Dataset as HFDataset, load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Assume project_path is defined from Cell 2 (Google Drive mount)\n",
        "project_path = '/content/drive/My Drive/EmotionalSupportBot-4' # <--- IMPORTANT: Update this to your new path!\n",
        "dataset_file_jsonl = os.path.join(project_path, \"emotional_support_structured_dataset.jsonl\")\n",
        "\n",
        "print(f\"Loading structured dataset from: {dataset_file_jsonl}\")\n",
        "\n",
        "# Load JSONL into Hugging Face Dataset\n",
        "loaded_hf_dataset_structured = load_dataset('json', data_files=dataset_file_jsonl, split='train')\n",
        "print(f\"Loaded {len(loaded_hf_dataset_structured)} structured samples.\")\n",
        "print(loaded_hf_dataset_structured[0]) # Print first sample to verify new structure\n",
        "\n",
        "# Now, map this structured dataset to create the 'text' column required for GPT-2 training\n",
        "def format_for_gpt2_training(example):\n",
        "    # This combines the fields into the \"User: ... [EMOTION: ...] Bot: ...\" format\n",
        "    return {\n",
        "        \"text\": f\"User: {example['user_input']} [EMOTION: {example['emotion']}] Bot: {example['assistant_response']}\"\n",
        "    }\n",
        "\n",
        "# Apply the formatting\n",
        "formatted_for_training_dataset = loaded_hf_dataset_structured.map(format_for_gpt2_training, remove_columns=['user_input', 'emotion', 'assistant_response'])\n",
        "\n",
        "# Extract the 'text' column AND EXPLICITLY CONVERT TO A PYTHON LIST\n",
        "formatted_texts_list = list(formatted_for_training_dataset['text']) # <--- KEY FIX HERE: added list() cast\n",
        "\n",
        "# Set a random seed for reproducibility (ensure SEED is defined, e.g., in Cell 1)\n",
        "# SEED = 42 # Assuming SEED is defined globally from Cell 1\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_texts, val_texts = train_test_split(formatted_texts_list, test_size=0.1, random_state=SEED)\n",
        "\n",
        "# Convert to Hugging Face Dataset objects\n",
        "train_dataset = HFDataset.from_pandas(pd.DataFrame({\"text\": train_texts}))\n",
        "val_dataset = HFDataset.from_pandas(pd.DataFrame({\"text\": val_texts}))\n",
        "\n",
        "print(f\"Training samples: {len(train_dataset)}\")\n",
        "print(f\"Validation samples: {len(val_dataset)}\")\n",
        "print(\"\\nExample formatted text for fine-tuning (from the combined 'text' field):\")\n",
        "print(train_dataset[0]['text'])"
      ],
      "metadata": {
        "id": "YvW60esWnqzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 5: Load Tokenizer and Model (Crucial Modifications)\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Define your stop token (must match what you added to the dataset)\n",
        "STOP_TOKEN = \"<|stop|>\"\n",
        "\n",
        "# Add the new stop token to the tokenizer's vocabulary.\n",
        "# This will assign it a new ID, typically len(tokenizer) BEFORE resizing.\n",
        "num_added_tokens = tokenizer.add_special_tokens({'additional_special_tokens': [STOP_TOKEN]})\n",
        "print(f\"Added {num_added_tokens} new token(s): '{STOP_TOKEN}'\")\n",
        "\n",
        "# Get the ID of your newly added stop token\n",
        "# This is crucial: it should be different from GPT-2's default EOS (50256)\n",
        "stop_token_id = tokenizer.convert_tokens_to_ids(STOP_TOKEN)\n",
        "print(f\"New stop token '{STOP_TOKEN}' assigned ID: {stop_token_id}\")\n",
        "\n",
        "# GPT-2 typically uses its EOS token for padding.\n",
        "# For better clarity and to avoid confusion with the original EOS,\n",
        "# let's explicitly set the pad_token to your new STOP_TOKEN if it's not already set,\n",
        "# or ensure it's distinct from the original GPT-2 EOS.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token_id = stop_token_id # Set padding to your new stop token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# IMPORTANT: Resize model embeddings to account for the new token.\n",
        "# This makes sure the model can learn an embedding for your new token.\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"Tokenizer loaded: {model_name}. Added '{STOP_TOKEN}' token with ID {stop_token_id}.\")\n",
        "print(f\"Model loaded: {model_name}. Embeddings resized to {len(tokenizer)} tokens.\")\n",
        "print(f\"Number of model parameters: {model.num_parameters() / 1e6:.2f}M\")"
      ],
      "metadata": {
        "id": "Wt7wu1K_nvf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 6: Tokenize and Prepare Data Loaders\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Ensure truncation and padding are handled\n",
        "    # max_length can be adjusted based on your typical sentence length\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "# Data Collator for Language Modeling will dynamically pad batches and create labels from inputs\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "print(\"\\nExample tokenized input (first sample):\")\n",
        "print(tokenized_train_dataset[0])"
      ],
      "metadata": {
        "id": "aIoFR_YRn6rX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 7: Configure Training Arguments and Trainer\n",
        "\n",
        "output_dir = os.path.join(project_path, \"gpt2_emotional_support_model\")\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3, # You might need more epochs depending on dataset size and complexity\n",
        "    per_device_train_batch_size=4, # Smaller batch size for CPU/limited GPU\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=8, # Effectively increases batch size to 4 * 8 = 32\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    adam_beta1=0.9,\n",
        "    adam_beta2=0.999,\n",
        "    adam_epsilon=1e-8,\n",
        "    max_grad_norm=1.0,\n",
        "    save_steps=500, # Save checkpoint every 500 steps\n",
        "    save_total_limit=2, # Only keep the last 2 checkpoints\n",
        "    eval_strategy=\"steps\", # Corrected argument name\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    log_level=\"info\",\n",
        "    seed=SEED,\n",
        "    # For CPU-only training, ensure no GPU-specific settings are enforced\n",
        "    no_cuda=True if not torch.cuda.is_available() else False, # Force CPU if no CUDA, otherwise use CUDA\n",
        "    report_to=\"none\" # Disable reporting to W&B etc. if not needed\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(f\"Model will be saved to: {output_dir}\")"
      ],
      "metadata": {
        "id": "lG_e4PEhn--z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 8: Train the Model\n",
        "print(\"Starting fine-tuning...\")\n",
        "trainer.train()\n",
        "print(\"Fine-tuning complete!\")\n",
        "\n",
        "# Save the final model\n",
        "final_model_path = os.path.join(output_dir, \"final_model\")\n",
        "trainer.save_model(final_model_path)\n",
        "tokenizer.save_pretrained(final_model_path)\n",
        "print(f\"Final fine-tuned model and tokenizer saved to: {final_model_path}\")"
      ],
      "metadata": {
        "id": "-oRHFum2oBqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Cell 9: Test the Fine-tuned Model (CPU inference) (Update pipeline and Cleaning)\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Assume final_model_path is defined from Cell 8\n",
        "final_model_path = '/content/drive/My Drive/EmotionalSupportBot-4/gpt2_emotional_support_model/final_model'\n",
        "\n",
        "loaded_tokenizer = AutoTokenizer.from_pretrained(final_model_path)\n",
        "loaded_model = AutoModelForCausalLM.from_pretrained(final_model_path)\n",
        "\n",
        "loaded_model.eval()\n",
        "device = \"cpu\"\n",
        "loaded_model.to(device)\n",
        "\n",
        "print(f\"Loaded model for inference on: {device}\")\n",
        "\n",
        "STOP_TOKEN = \"<|stop|>\"\n",
        "# Ensure you get the ID from the loaded tokenizer, which now should have the unique ID\n",
        "stop_token_id = loaded_tokenizer.convert_tokens_to_ids(STOP_TOKEN)\n",
        "# Fallback just in case, though it should be set correctly if Cell 5 ran.\n",
        "if loaded_tokenizer.pad_token_id is None:\n",
        "    loaded_tokenizer.pad_token_id = stop_token_id\n",
        "print(f\"Stop token '{STOP_TOKEN}' ID: {stop_token_id}\")\n",
        "\n",
        "# --- KEY CHANGE IN PIPELINE INSTANTIATION ---\n",
        "# Crucial: explicitly pass eos_token_id and pad_token_id to the pipeline.\n",
        "# Ensure these are the IDs of your custom STOP_TOKEN.\n",
        "generator = pipeline(\n",
        "    'text-generation',\n",
        "    model=loaded_model,\n",
        "    tokenizer=loaded_tokenizer,\n",
        "    device=-1, # -1 specifies CPU\n",
        "    eos_token_id=stop_token_id, # Tell it to stop explicitly at this token\n",
        "    pad_token_id=stop_token_id  # Use the same for padding\n",
        ")\n",
        "# --- END KEY CHANGE ---\n",
        "\n",
        "\n",
        "def generate_response(prompt_text, max_new_tokens=60, num_return_sequences=1, assumed_emotion=None):\n",
        "    if assumed_emotion:\n",
        "        input_prompt = f\"User: {prompt_text} [EMOTION: {assumed_emotion}] Bot:\"\n",
        "    else:\n",
        "        input_prompt = f\"User: {prompt_text} Bot:\"\n",
        "\n",
        "    print(f\"\\nGenerating response for: '{prompt_text}' (Assumed Emotion: {assumed_emotion if assumed_emotion else 'None'})\")\n",
        "\n",
        "    generated_sequences = generator(\n",
        "        input_prompt,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        no_repeat_ngram_size=5, # Keep at 5 or slightly higher\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.9,\n",
        "        do_sample=True,\n",
        "        # eos_token_id and pad_token_id are now set in the pipeline constructor,\n",
        "        # and do not need to be repeated here unless you want to override.\n",
        "    )\n",
        "\n",
        "    for i, sequence in enumerate(generated_sequences):\n",
        "        generated_text = sequence['generated_text']\n",
        "        bot_response_start = generated_text.find(\"Bot:\")\n",
        "        if bot_response_start != -1:\n",
        "            clean_response = generated_text[bot_response_start + len(\"Bot:\"):].strip()\n",
        "\n",
        "            # --- MORE AGGRESSIVE CLEANUP FOR STOP TOKENS AND ARTIFACTS ---\n",
        "            # Remove all occurrences of the STOP_TOKEN and common HTML-like variants\n",
        "            clean_response = clean_response.replace(STOP_TOKEN, '').strip()\n",
        "            clean_response = clean_response.replace('<|stop|>', '').strip()\n",
        "            clean_response = clean_response.replace('</stop|>', '').strip() # The HTML-like variant\n",
        "            clean_response = clean_response.replace('>', '').strip() # Catch lingering > characters\n",
        "            clean_response = clean_response.replace('<', '').strip() # Catch lingering < characters\n",
        "            clean_response = clean_response.replace('|', '').strip() # Catch lingering | characters\n",
        "\n",
        "            # Remove any residual emotion tags or partial tags\n",
        "            clean_response = clean_response.split('[EMOTION:')[0].strip()\n",
        "            clean_response = clean_response.split('[EMOTE:')[0].strip() # Handles potentially different tags\n",
        "\n",
        "            # Ensure it stops at a logical break (newline or start of next user/bot turn)\n",
        "            clean_response = clean_response.split('User:')[0].strip()\n",
        "            clean_response = clean_response.split('Bot:')[0].strip()\n",
        "            clean_response = clean_response.split('\\n')[0].strip()\n",
        "\n",
        "            # Final punctuation cleanup\n",
        "            if clean_response and not (clean_response.endswith('.') or clean_response.endswith('?') or clean_response.endswith('!')):\n",
        "                # Only add ellipsis if it seems cut off mid-sentence and is not too short\n",
        "                if len(clean_response.split()) > 3 and not clean_response.endswith('...'):\n",
        "                     clean_response += \"...\"\n",
        "                elif len(clean_response) > 0: # If there's some text, ensure it ends with period\n",
        "                    clean_response += \".\"\n",
        "            # --- END MORE AGGRESSIVE CLEANUP ---\n",
        "\n",
        "            print(f\"Generated Response {i+1}: {clean_response}\")\n",
        "        else:\n",
        "            print(f\"Generated Raw Text {i+1}: {generated_text}\")\n",
        "\n",
        "# Test calls (set num_return_sequences=1 in pipeline constructor for single output per prompt)\n",
        "generate_response(\"I'm feeling so happy today, it's amazing!\", max_new_tokens=40, assumed_emotion=\"joy\", num_return_sequences=1)\n",
        "generate_response(\"I just got some really bad news and I feel utterly devastated.\", max_new_tokens=40, assumed_emotion=\"sadness\", num_return_sequences=1)\n",
        "generate_response(\"I'm so angry at how things turned out!\", max_new_tokens=40, assumed_emotion=\"anger\", num_return_sequences=1)\n",
        "generate_response(\"I'm quite worried about my exam results.\", max_new_tokens=40, assumed_emotion=\"fear\", num_return_sequences=1)\n",
        "generate_response(\"This is so boring, I don't know what to do.\", max_new_tokens=40, assumed_emotion=\"neutral\", num_return_sequences=1)\n",
        "generate_response(\"I feel incredibly grateful for your support.\", max_new_tokens=40, assumed_emotion=\"gratitude\", num_return_sequences=1)"
      ],
      "metadata": {
        "id": "v-X-EHJroDnB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}